{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModelForTokenClassification, TFTrainer, TFTrainingArguments\n",
    "from transformers.utils import logging as hf_logging\n",
    "from seqeval.metrics.sequence_labeling import performance_measure, precision_recall_fscore_support, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioes = True # BIOES/BIO labels\n",
    "task1 = True # Task1 / Task2\n",
    "plus = True # Use ProfNER for Task 1\n",
    "positives = True # Only positives for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_seq_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Brat annotation format to BIO/BIOES tag schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(files):\n",
    "    labels = set()\n",
    "    for file_name in files:\n",
    "        if os.path.exists(file_name + \".ann\"):\n",
    "            with open(file_name + \".ann\", \"rb\") as file:\n",
    "                annotations = file.read().decode().splitlines()\n",
    "            for ann in annotations:\n",
    "                if ann[0] == 'T':\n",
    "                    labels.add(ann.split(\"\\t\", maxsplit=2)[1].split(\" \", maxsplit=1)[0])\n",
    "    return list(labels)\n",
    "\n",
    "def divide_text(text, max_seq_length):\n",
    "    # Divide into multiple sentences until words > max_seq_length\n",
    "    sentences = []\n",
    "    text_sentences = nlp(text)\n",
    "    maximum = 0\n",
    "    initial = 0\n",
    "    for sentence in text_sentences.sents:\n",
    "        len_sentence = sum([len(tokenizer._tokenizer.encode_batch([word.text], add_special_tokens=False)[0].tokens) for word in sentence])\n",
    "        maximum += len_sentence\n",
    "        if maximum >= max_seq_length:\n",
    "            # Agregate current sentence if greater than maximum\n",
    "            if len_sentence >= max_seq_length:\n",
    "                # Agregate previous sentences\n",
    "                if maximum != len_sentence:\n",
    "                    final = sentence.start_char\n",
    "                    sentences.append([initial, final])\n",
    "                    initial = final\n",
    "                maximum = 0\n",
    "                final = sentence.end_char\n",
    "            else:\n",
    "                maximum = len_sentence\n",
    "                final = sentence.start_char\n",
    "            sentences.append([initial, final])\n",
    "            initial = final\n",
    "    # Agregate the last sentences\n",
    "    if initial != len(text):\n",
    "        sentences.append([initial, len(text)])\n",
    "    return sentences\n",
    "\n",
    "def brat2BIO(file_name, labels, bioes=True, folder=\"\"):\n",
    "    # Load text and annotation files\n",
    "    with open(file_name + \".txt\", \"rb\") as file:\n",
    "        text = file.read().decode()\n",
    "    entities = []\n",
    "    if os.path.exists(file_name + \".ann\"):\n",
    "        with open(file_name + \".ann\", \"rb\") as file:\n",
    "            annotations = file.read().decode().splitlines()\n",
    "        # Extract the entities from the annotation file\n",
    "        for ann in annotations:\n",
    "            if ann[0] == 'T':\n",
    "                idx, entity_label, name = ann.split(\"\\t\", maxsplit=2)\n",
    "                entity_label, offsets = entity_label.split(\" \", maxsplit=1)\n",
    "                offsets = [[int(offset) for offset in pair.split(\" \")] for pair in offsets.split(';')]\n",
    "                off_start, off_end = [min(offset[0] for offset in offsets), max(offset[1] for offset in offsets)]\n",
    "                entities.append([int(idx[1:]), entity_label, off_start, off_end, text[off_start:off_end]])\n",
    "        # Deoverlap nested entities for each class\n",
    "        ents = []\n",
    "        for label in labels:\n",
    "            offsets = [[e[2],e[3]] for e in entities if e[1]==label]\n",
    "            overlaps = [[o1, [o2 for o2 in offsets if not o1 == o2 and o1[0] < o2[1] and o1[1] > o2[0]]] for o1 in offsets]\n",
    "            if [o for o in overlaps if o[1]]:\n",
    "                ranges = [max([overlap[0]]+[o for o in overlap[1]], key=lambda x: x[1]-x[0]) for overlap in overlaps]\n",
    "                offsets = [[r[0],r[1]] for r in set([(r[0],r[1]) for r in ranges])]\n",
    "            ents.append(offsets)\n",
    "        entities = ents\n",
    "    # Clean the texts\n",
    "    text = text.lower().replace(\"\\n\",\" \")\n",
    "    # Divide text into sentences\n",
    "    sentences = divide_text(text, max_seq_length)\n",
    "    # Annotate each token with BIO/BIOES schema    \n",
    "    for l, label in enumerate(labels):\n",
    "        for start_sentence, end_sentence in sentences:\n",
    "            new_tokens = []\n",
    "            for word in nlp(text[start_sentence:end_sentence]):\n",
    "                tokenized = tokenizer._tokenizer.encode_batch([word.text], add_special_tokens=False)[0]\n",
    "                for token, offset in zip(tokenized.tokens, tokenized.offsets):\n",
    "                    if token.startswith(\"##\"):\n",
    "                        new_tokens[-1][1] = word.idx+offset[1]\n",
    "                    else:\n",
    "                        new_tokens.append([word.idx+offset[0], word.idx+offset[1]])\n",
    "            new_tokens = [[text[start_sentence:end_sentence][token_start:token_end], token_start, token_end, \"O\"] for token_start, token_end in new_tokens]\n",
    "            if entities:\n",
    "                tokens = []\n",
    "                offsets = [(start_entity-start_sentence, end_entity-start_sentence) for start_entity, end_entity in entities[l] if start_sentence<=start_entity and end_sentence>=end_entity]\n",
    "                for token_text, token_start, token_end, token_label in new_tokens:\n",
    "                    # Check if the token is annotated\n",
    "                    for entity_start, entity_end in offsets:\n",
    "                        # Match between token and entity offsets\n",
    "                        if token_start < entity_end and token_end > entity_start:\n",
    "                            if token_start == entity_start and token_end == entity_end:\n",
    "                                token_label = (\"S-\" if bioes else \"B-\") + label\n",
    "                            elif token_start == entity_start and token_end < entity_end:\n",
    "                                token_label = \"B-\" + label\n",
    "                            elif token_start > entity_start and token_end == entity_end:\n",
    "                                token_label = (\"E-\" if bioes else \"I-\") + label\n",
    "                            else: # token_start > entity_start and token_end < entity_end\n",
    "                                token_label = \"I-\" + label\n",
    "                            break\n",
    "                    tokens.append([token_text, token_start, token_end, token_label])\n",
    "            else:\n",
    "                tokens = new_tokens\n",
    "            pd.DataFrame(tokens, columns = [\"Token\",\"Offset start\",\"Offset end\",\"Label\"]).to_csv(os.path.join(folder, file_name.split(\"\\\\\")[-1] + \"~\" + label + \"~\" + str(start_sentence) + \".csv\"), index=False, encoding=\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Task 1\n",
    "folder = os.path.join(\"meddoprof_training_set\", \"task1\")\n",
    "if os.path.exists(folder):\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    labels_task1 = get_labels(files)\n",
    "    print(folder + \" (documents = \" + str(len(files)) + \"):\")\n",
    "\n",
    "# Training set\n",
    "folderTrain = \"processedTrain_task1\"\n",
    "if not os.path.exists(folderTrain):\n",
    "    os.mkdir(folderTrain)\n",
    "    folder = os.path.join(\"meddoprof_training_set\", \"task1\")\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task1, bioes=bioes, folder=folderTrain)\n",
    "\n",
    "# Training and Valid sets PROFNER\n",
    "folderTrain = \"processedTrain_task1_plus\"\n",
    "if not os.path.exists(folderTrain):\n",
    "    os.mkdir(folderTrain)\n",
    "    folder = os.path.join(\"ProfNER\",\"DATA\", \"subtask-2\", \"brat\", \"train\")\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task1, bioes=bioes, folder=folderTrain)\n",
    "    folder = os.path.join(\"ProfNER\",\"DATA\", \"subtask-2\", \"brat\", \"valid\")\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task1, bioes=bioes, folder=folderTrain)\n",
    "\n",
    "# Test set\n",
    "folderTest = \"processedTest_task1\"\n",
    "if not os.path.exists(folderTest):\n",
    "    os.mkdir(folderTest)\n",
    "    folder = \"meddoprof_test_set\"\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task1, bioes=bioes, folder=folderTest)\n",
    "\n",
    "# Labels Task 2\n",
    "folder = os.path.join(\"meddoprof_training_set\", \"task2\")\n",
    "if os.path.exists(folder):\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    labels_task2 = get_labels(files)\n",
    "    print(folder + \" (documents = \" + str(len(files)) + \"):\")\n",
    "\n",
    "# Training set\n",
    "folderTrain = \"processedTrain_task2\"\n",
    "if not os.path.exists(folderTrain):\n",
    "    os.mkdir(folderTrain)\n",
    "    folder = os.path.join(\"meddoprof_training_set\", \"task2\")\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task2, bioes=bioes, folder=folderTrain)\n",
    "\n",
    "# Test set\n",
    "folderTest = \"processedTest_task2\"\n",
    "if not os.path.exists(folderTest):\n",
    "    os.mkdir(folderTest)\n",
    "    folder = \"meddoprof_test_set\"\n",
    "    files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "    for file_name in tqdm(files):\n",
    "        brat2BIO(file_name, labels_task2, bioes=bioes, folder=folderTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(csv_files, tokenizer, max_seq_length, model_name, label2id):\n",
    "    # Initialize feature list\n",
    "    features = []\n",
    "    # Maximum sequence length\n",
    "    special_tokens = tokenizer.num_special_tokens_to_add()\n",
    "    # Special tokens\n",
    "    cls_token_id = tokenizer.cls_token_id\n",
    "    sep_token_id = tokenizer.sep_token_id\n",
    "    cls_token_at_end = \"xlnet\" in model_name.lower()\n",
    "    sep_token_extra = \"roberta\" in model_name.lower()\n",
    "    # IDs for mask and token type\n",
    "    token_mask_id = 1#tokenizer.token_mask_id\n",
    "    token_type_id = 0#tokenizer.token_type_id\n",
    "    # IDs for padding\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    pad_token_label_id = -100#tokenizer.pad_token_label_id\n",
    "    pad_token_mask_id = 0#tokenizer.pad_token_mask_id\n",
    "    pad_token_type_id = tokenizer.pad_token_type_id\n",
    "    left_pad = tokenizer.padding_side==\"left\"\n",
    "    right_pad = tokenizer.padding_side==\"right\"\n",
    "    for csv_file in tqdm(csv_files):\n",
    "        # Initialize word and label ids\n",
    "        input_ids = []\n",
    "        label_ids = []\n",
    "        # Tokenize each word in the csv file\n",
    "        for word, label in pd.read_csv(csv_file, keep_default_na=False, encoding = \"utf16\")[[\"Token\",\"Label\"]].values.tolist():\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            if len(tokens) > 0:\n",
    "                input_ids += tokenizer.convert_tokens_to_ids(tokens)\n",
    "                label_ids += [label2id[label]] + [pad_token_label_id]*(len(tokens)-1)#[label2id[label]]*len(tokens)\n",
    "        # Keep only maximum sequence length\n",
    "        input_ids = input_ids[:(max_seq_length-special_tokens)]\n",
    "        label_ids = label_ids[:(max_seq_length-special_tokens)]\n",
    "        # Add special tokens to the features\n",
    "        input_ids = [cls_token_id]*(not cls_token_at_end) + input_ids + [sep_token_id] + [sep_token_id]*sep_token_extra + [cls_token_id]*cls_token_at_end\n",
    "        label_ids = [pad_token_label_id]*(not cls_token_at_end) + label_ids + [pad_token_label_id] + [pad_token_label_id]*sep_token_extra + [pad_token_label_id]*cls_token_at_end\n",
    "        # Initialize attention mask and token type ids\n",
    "        attention_mask = [token_mask_id]*len(input_ids)\n",
    "        token_type_ids = [token_type_id]*len(input_ids)\n",
    "        # Pad to the maximum sequence length\n",
    "        padding_length = max_seq_length-len(input_ids)\n",
    "        input_ids = ([pad_token_id]*padding_length)*left_pad + input_ids + ([pad_token_id]*padding_length)*right_pad\n",
    "        label_ids = ([pad_token_label_id]*padding_length)*left_pad + label_ids + ([pad_token_label_id]*padding_length)*right_pad\n",
    "        attention_mask = ([pad_token_mask_id]*padding_length)*left_pad + attention_mask + ([pad_token_mask_id]*padding_length)*right_pad\n",
    "        token_type_ids = ([pad_token_type_id]*padding_length)*left_pad + token_type_ids + ([pad_token_type_id]*padding_length)*right_pad\n",
    "        # Add example to feature list\n",
    "        feature = {\"input_ids\": input_ids}\n",
    "        if \"attention_mask\" in tokenizer.model_input_names:\n",
    "            feature[\"attention_mask\"] = attention_mask\n",
    "        if \"token_type_ids\" in tokenizer.model_input_names:\n",
    "            feature[\"token_type_ids\"] = token_type_ids\n",
    "        features.append([feature, label_ids])\n",
    "    # Features to tensors\n",
    "    def gen():\n",
    "        for feature, label in features:\n",
    "            yield (feature, label)\n",
    "    tf_types = {\"input_ids\": tf.int32}\n",
    "    tf_shapes = {\"input_ids\": tf.TensorShape([None])}\n",
    "    if \"attention_mask\" in tokenizer.model_input_names:\n",
    "        tf_types[\"attention_mask\"] = tf.int32\n",
    "        tf_shapes[\"attention_mask\"] = tf.TensorShape([None])\n",
    "    if \"token_type_ids\" in tokenizer.model_input_names:\n",
    "        tf_types[\"token_type_ids\"] = tf.int32\n",
    "        tf_shapes[\"token_type_ids\"] = tf.TensorShape([None])\n",
    "    dataset = tf.data.Dataset.from_generator(gen, (tf_types, tf.int64), (tf_shapes, tf.TensorShape([None])))\n",
    "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(features)))\n",
    "    return dataset\n",
    "\n",
    "def filter_predictions(predictions, label_ids):\n",
    "    preds = tf.argmax(predictions, axis=2).numpy()\n",
    "    batch_size, seq_len = preds.shape\n",
    "    y_true = [[id2label[label_ids[i][j]] for j in range(seq_len) if label_ids[i][j] != -100] for i in range(batch_size)]\n",
    "    y_pred = [[id2label[preds[i][j]] for j in range(seq_len) if label_ids[i][j] != -100] for i in range(batch_size)]\n",
    "    return y_true, y_pred\n",
    "\n",
    "def compute_metrics(predictions):\n",
    "    y_true, y_pred = filter_predictions(predictions.predictions, predictions.label_ids)\n",
    "    performance = performance_measure(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
    "    return {\n",
    "        \"true_positives\": performance[\"TP\"],\n",
    "        \"true_negatives\": performance[\"TN\"],\n",
    "        \"false_positives\": performance[\"FP\"],\n",
    "        \"false_negatives\": performance[\"FN\"],\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir=\"MEDDOPROF\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    num_train_epochs=8.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(training_args.seed)\n",
    "\n",
    "labels = labels_task1 if task1 else labels_task2\n",
    "labels_names = [[\"O\", \"B-\"+label, \"I-\"+label, \"E-\"+label, \"S-\"+label] if bioes else [\"O\", \"B-\"+label, \"I-\"+label] for label in labels]\n",
    "id2labels = [{i: l for i, l in enumerate(label)} for label in labels_names]\n",
    "labels2id = [{l: i for i, l in enumerate(label)} for label in labels_names]\n",
    "\n",
    "# Training set\n",
    "folder = \"processedTrain_task1\" if task1 else \"processedTrain_task2\"\n",
    "csv_files_train = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".csv\")]\n",
    "# Training set plus\n",
    "folder = \"processedTrain_task1_plus\"\n",
    "if task1 and plus:\n",
    "    csv_files_train += [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".csv\")]\n",
    "random.shuffle(csv_files_train)\n",
    "csv_files_train = [[csv_file_train for csv_file_train in csv_files_train if label in csv_file_train] for label in labels]\n",
    "\n",
    "# Test set\n",
    "folder = \"processedTest_task1\" if task1 else \"processedTest_task2\"\n",
    "csv_files_predict = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".csv\")]\n",
    "random.shuffle(csv_files_predict)\n",
    "csv_files_predict = [[csv_file_predict for csv_file_predict in csv_files_predict if label in csv_file_predict] for label in labels]\n",
    "\n",
    "# Development set\n",
    "PER_EVAL = 0.2\n",
    "csv_files_pos = [[csv_file_train for csv_file_train in csv_files_train[l] if label in set([i.split(\"-\")[-1] for i in set(pd.read_csv(csv_file_train, keep_default_na=False, encoding = \"utf16\")[\"Label\"].values.tolist())])] for l, label in enumerate(labels)]\n",
    "csv_files_neg = [[csv_file_train for csv_file_train in csv_files_train[l] if not label in set([i.split(\"-\")[-1] for i in set(pd.read_csv(csv_file_train, keep_default_na=False, encoding = \"utf16\")[\"Label\"].values.tolist())])] for l, label in enumerate(labels)]\n",
    "csv_files_eval = [csv_files_pos[l][:int(len(csv_files_pos[l])*PER_EVAL)]+csv_files_neg[l][:int(len(csv_files_neg[l])*PER_EVAL)] for l in range(len(labels))]\n",
    "csv_files_train = [csv_files_pos[l][int(len(csv_files_pos[l])*PER_EVAL):]+([] if positives else csv_files_neg[l][int(len(csv_files_neg[l])*PER_EVAL):]) for l in range(len(labels))]\n",
    "[random.shuffle(csv_file_eval) for csv_file_eval in csv_files_eval]\n",
    "[random.shuffle(csv_file_train) for csv_file_train in csv_files_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "hf_logging.set_verbosity_info()\n",
    "hf_logging.enable_default_handler()\n",
    "hf_logging.enable_explicit_format()\n",
    "\n",
    "# Prepare Token Classification task\n",
    "for l, label in enumerate(labels):\n",
    "    print(\"######## \" + os.path.join(\"MEDDOPROF_\" + model_name.replace(\"/\",\"_\"), label) + \" ########\")\n",
    "    training_args.output_dir = os.path.join(\"MEDDOPROF_\" + model_name.replace(\"/\",\"_\"), label)\n",
    "    id2label = id2labels[l]\n",
    "    label2id = labels2id[l]\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=len(labels), id2label=id2label, label2id=label2id, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "    with training_args.strategy.scope():\n",
    "        model = TFAutoModelForTokenClassification.from_pretrained(model_name, config=config, from_pt=True)\n",
    "    \n",
    "    # Get datasets\n",
    "    train_dataset = extract_dataset(csv_files_train[l], tokenizer, max_seq_length, model_name, label2id) if training_args.do_train else None\n",
    "    eval_dataset = extract_dataset(csv_files_eval[l], tokenizer, max_seq_length, model_name, label2id) if training_args.do_eval else None\n",
    "    test_dataset = extract_dataset(csv_files_predict[l], tokenizer, max_seq_length, model_name, label2id) if training_args.do_predict else None\n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = TFTrainer(model=model, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics, args=training_args)\n",
    "    \n",
    "    # Training set\n",
    "    if training_args.do_train:\n",
    "        approx = math.floor if training_args.dataloader_drop_last else math.ceil\n",
    "        steps_per_epoch = max(approx(tf.data.experimental.cardinality(train_dataset).numpy()/(training_args.train_batch_size*training_args.gradient_accumulation_steps)),1)\n",
    "        training_args.logging_steps = int(steps_per_epoch/4)        \n",
    "        training_args.eval_steps = steps_per_epoch\n",
    "        training_args.save_steps = steps_per_epoch\n",
    "        trainer.train()\n",
    "    \n",
    "    # Development set\n",
    "    if training_args.do_eval:\n",
    "        # Results for every epoch\n",
    "        results = []\n",
    "        with training_args.strategy.scope():\n",
    "            ckpt = tf.train.Checkpoint(optimizer=trainer.optimizer, model=trainer.model)\n",
    "            for checkpoint in trainer.model.ckpt_manager.checkpoints:\n",
    "                ckpt.restore(checkpoint)\n",
    "                results.append(trainer.evaluate())\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        criteria = [result[\"eval_loss\"] for result in results]\n",
    "        checkpoint = trainer.model.ckpt_manager.checkpoints[criteria.index(min(criteria))]\n",
    "        trainer.save_model()\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "        \n",
    "        # Save Development results\n",
    "        with open(os.path.join(training_args.output_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key, value in results[criteria.index(min(criteria))].items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "    \n",
    "    # Test set\n",
    "    if training_args.do_predict:\n",
    "        # Load best checkpoint\n",
    "        with training_args.strategy.scope():\n",
    "            ckpt = tf.train.Checkpoint(optimizer=trainer.optimizer, model=trainer.model)\n",
    "            ckpt.restore(checkpoint if checkpoint else trainer.model.ckpt_manager.latest_checkpoint)\n",
    "        \n",
    "        # Final results\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        y_true, y_pred = filter_predictions(predictions, label_ids)\n",
    "        \n",
    "        # Save predictions\n",
    "        folder = os.path.join(training_args.output_dir, \"prediction\")\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        \n",
    "        tokens = [tokenizer.convert_ids_to_tokens(test_data[0][\"input_ids\"]) for test_data in iter(test_dataset)]\n",
    "        preds = tf.argmax(predictions, axis=2).numpy()\n",
    "        \n",
    "        for file, [token_data, pred_data] in enumerate(zip(tokens, preds)):\n",
    "            new_tokens = []\n",
    "            new_y_pred = []\n",
    "            for token, pred in zip(token_data, pred_data):\n",
    "                if token==\"[SEP]\":\n",
    "                    break\n",
    "                if token!=\"[CLS]\":\n",
    "                    if token.startswith(\"##\"):\n",
    "                        new_tokens[-1] += token[2:]\n",
    "                    else:\n",
    "                        new_tokens.append(token)\n",
    "                        new_y_pred.append(id2label[pred])\n",
    "            csv_file_predict = csv_files_predict[l][file]\n",
    "            offset_start, offset_end = zip(*pd.read_csv(csv_file_predict, keep_default_na=False, encoding=\"utf-16\")[[\"Offset start\",\"Offset end\"]].values.tolist())\n",
    "            csv_prediction = os.path.join(folder, os.path.split(csv_file_predict)[-1])\n",
    "            pd.DataFrame(list(zip(new_tokens, offset_start, offset_end, new_y_pred)), columns = [\"Token\",\"Offset start\",\"Offset end\",\"Label\"]).to_csv(csv_prediction, index=False, encoding=\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO2brat(csv_files, folder=\"\"):\n",
    "    annotations = []\n",
    "    for csv_file in csv_files:\n",
    "        offset = int(os.path.split(csv_file)[-1].split(\"~\")[-1][:-4])\n",
    "        previous = \"O\"\n",
    "        for _, offset_start, offset_end, label in pd.read_csv(csv_file, keep_default_na=False, encoding=\"utf-16\").values.tolist():\n",
    "            if label != \"O\":\n",
    "                if previous != \"O\" and label[0] not in [\"B\",\"S\"]:\n",
    "                    annotations[-1][-1] = offset_end+offset\n",
    "                else:\n",
    "                    annotations.append([label[2:], offset_start+offset, offset_end+offset])\n",
    "            previous = label\n",
    "    annotations = list(set([(label, offset_start, offset_end) for label, offset_start, offset_end in annotations]))\n",
    "    annotations.sort(key = lambda x: x[1])\n",
    "    annotations = [\"T\"+str(i+1)+\"\\t\"+label+\" \"+str(offset_start)+\" \"+str(offset_end) for i, [label, offset_start, offset_end] in enumerate(annotations)]\n",
    "    with open(os.path.join(folder, os.path.split(csv_files[0])[-1].split(\"~\")[0] + \"_prediction.ann\"), \"wb\") as file:\n",
    "        file.write(\"\\n\".join(annotations).encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(\"MEDDOPROF_\" + model_name.replace(\"/\",\"_\"), \"prediction\")\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "csv_prediction_files = [[os.path.join(folder, file) for folder in [os.path.join(\"MEDDOPROF_\" + model_name.replace(\"/\",\"_\"), label, \"prediction\")] for file in os.listdir(folder) if file.endswith(\".csv\")] for label in labels]\n",
    "csv_prediction_set = set([os.path.split(csv_prediction_file)[-1].split(\"~\")[0] for label in csv_prediction_files for csv_prediction_file in label])\n",
    "csv_prediction_files = [[csv_prediction_file for label in csv_prediction_files for csv_prediction_file in label if csv_prediction_num == os.path.split(csv_prediction_file)[-1].split(\"~\")[0]] for csv_prediction_num in csv_prediction_set]\n",
    "\n",
    "for csv_prediction_file in tqdm(csv_prediction_files):\n",
    "    BIO2brat(csv_prediction_file, folder=folder)\n",
    "predictions = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".ann\")]\n",
    "\n",
    "folder = \"meddoprof_test_set\"\n",
    "files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".txt\")]\n",
    "\n",
    "# Aggregate text into annotations\n",
    "for file_name, prediction in tqdm([(file_name, prediction) for file_name in files for prediction in predictions if os.path.split(file_name)[-1].split(\".\")[0] == os.path.split(prediction)[-1].split(\"_prediction\")[0]]):\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        text = file.read().decode()\n",
    "    with open(prediction, \"rb\") as file:\n",
    "        annotations = file.read().decode().splitlines()\n",
    "    annotations = [ann+\"\\t\"+text[int(off_start):int(off_end)] for ann in annotations for off_start, off_end in [ann.split(\"\\t\", maxsplit=2)[-1].split(\" \", maxsplit=1)[-1].split(\" \")]]\n",
    "    with open(prediction, \"wb\") as file:\n",
    "        file.write(\"\\n\".join(annotations).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv(\"meddoprof_valid_codes.tsv\", sep=\"\\t\", keep_default_na=False)\n",
    "dictionary = {label: code for code, labels, alternatives in codes.values.tolist() for label in labels.split(\"|\")}\n",
    "dictionary.update({alternative: code for code, labels, alternatives in codes.values.tolist() for alternative in alternatives.split(\"|\")})\n",
    "\n",
    "folder = os.path.join(\"MEDDOPROF_\" + model_name.replace(\"/\",\"_\"), \"prediction\")\n",
    "files = [os.path.join(folder, file)[:-4] for file in os.listdir(folder) if file.endswith(\".ann\")]\n",
    "files_codes = []\n",
    "for file_name in files:\n",
    "    with open(file_name + \".ann\", \"rb\") as file:\n",
    "        annotations = file.read().decode().splitlines()\n",
    "    for ann in annotations:\n",
    "        if ann and ann[0] == 'T':\n",
    "            idx, entity_label, name = ann.split(\"\\t\", maxsplit=2)\n",
    "            entity_label, offsets = entity_label.split(\" \", maxsplit=1)\n",
    "            offsets = [[int(offset) for offset in pair.split(\" \")] for pair in offsets.split(';')]\n",
    "            off_start, off_end = [min(offset[0] for offset in offsets), max(offset[1] for offset in offsets)]\n",
    "            files_codes.append([os.path.split(file_name)[-1],name,str(off_start)+\" \"+str(off_end)])\n",
    "final_file = files_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = lambda u,v: nltk.distance.edit_distance(u, v, transpositions=True)\n",
    "string_matchings = [[value for key, value in dictionary.items() if key==name] for filename, name, span in files_codes]\n",
    "string_matchings = [string_matching[0] if string_matching else min([[value, similarity(files_codes[s][1], key)] for key, value in dictionary.items()], key=lambda x: x[1])[0] for s, string_matching in tqdm(enumerate(string_matchings))]\n",
    "final_file = pd.DataFrame([[\"\\\"\"+filename+\"\\\"\", \"\\\"\"+name+\"\\\"\", \"\\\"\"+span+\"\\\"\", \"\\\"\"+string_matching+\"\\\"\"] for [filename, name, span], string_matching in zip(final_file, string_matchings)], columns=[\"\\\"filename\\\"\", \"\\\"text\\\"\", \"\\\"span\\\"\", \"\\\"code\\\"\"])\n",
    "final_file.to_csv(os.path.join(os.path.split(folder)[0], \"task3.tsv\"), index=False, encoding=\"utf-16\", sep='\\t',quotechar=\"'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
